{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y_{in} = F_{act}((Im+n_{in} W_{in}) + bias) $$\n",
    "$$ Im^* = F_{act}(y_{in} W_{in}^T) + n_{out} $$\n",
    "\n",
    "$$ Cost = \\sqrt{\\langle|Im-Im^*|\\rangle} + \\lambda \\langle r \\rangle $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils.plotutils as plu\n",
    "import utils.imreadin as imr\n",
    "import utils.animutils as aniu\n",
    "#import utils.dirutils as diru\n",
    "\n",
    "#code to reload\n",
    "#import imp\n",
    "#imp.reload(dir)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs = {'dpi' : 200} #plotting pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class aec_model(object):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "      params = self.add_params(params)\n",
    "      self.params = params\n",
    "      self.make_dirs()\n",
    "      self.make_graph()\n",
    "    \n",
    "    def add_params(self, params):   \n",
    "        params['nneurons'] = np.int(params['imxlen']*params['imylen']/params['compression'])\n",
    "        params['savefolder'] = str('c/actfun_'+ params['model_type']+\n",
    "                                   '/compression_'+ str(params['compression'])+\n",
    "                                   '/noise_in_'+ str(params['noise_x'])+\n",
    "                                   '_out_'+ str(params['noise_r'])+\n",
    "                                   '_lambda_'+ str(params['lambd'])+'/')\n",
    "        return(params)\n",
    "        \n",
    "    def make_dirs(self):\n",
    "        if not os.path.exists(self.params['savefolder']):\n",
    "            os.makedirs(self.params['savefolder'])\n",
    "        \n",
    "    def make_graph(self):\n",
    "    \n",
    "        print('Compressing by',self.params['compression'],'for a total of',self.params['nneurons'],'neurons')\n",
    "\n",
    "        #setup our graph\n",
    "        #tf.reset_default_graph()\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "\n",
    "            #input images\n",
    "            with tf.name_scope('input'):\n",
    "                self.x = tf.placeholder(tf.float32, shape=[batchsize, imxlen*imylen])\n",
    "\n",
    "            #activation function type\n",
    "            with tf.name_scope('activation_function'):\n",
    "                self.act_fun = tf.placeholder(tf.string)\n",
    "\n",
    "            #noises\n",
    "            with tf.name_scope('noises'):\n",
    "                self.noisexsigma = tf.placeholder(tf.float32)\n",
    "                self.noisersigma = tf.placeholder(tf.float32)\n",
    "\n",
    "            #function to add noise\n",
    "            with tf.name_scope(\"add_noise\"):\n",
    "                def add_noise(input_layer, std):\n",
    "                    noise = tf.random_normal(shape=tf.shape(input_layer), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "                    return tf.add(input_layer,noise)\n",
    "\n",
    "            #weights\n",
    "            with tf.variable_scope(\"weights\"):\n",
    "                self.win = tf.Variable(tf.random_normal([self.params['imxlen']*self.params['imylen'],\n",
    "                                                         self.params['nneurons']],\n",
    "                                                        dtype=tf.float32,stddev=0.1))\n",
    "                #wout = tf.Variable(tf.random_normal([nneurons,imxlen*imylen],dtype=tf.float32,stddev=0.01),name='weights_out')\n",
    "\n",
    "            #bias\n",
    "            with tf.variable_scope(\"bias\"):\n",
    "                self.bias = tf.Variable(tf.random_normal([self.params['nneurons']],dtype=tf.float32,stddev=0.1))\n",
    "\n",
    "            #lambda\n",
    "            with tf.name_scope('lambda'):\n",
    "                self.lambd = tf.placeholder(tf.float32)\n",
    "\n",
    "            #learning_rate\n",
    "            with tf.name_scope('learning_rate'):\n",
    "                self.learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "            #nonlienarities\n",
    "            with tf.name_scope(\"nonlienarities\"):\n",
    "                #define nonlinearities\n",
    "                def tanh_fun(arg):\n",
    "                    return tf.nn.tanh(arg) \n",
    "                def sigmoid_fun(arg):\n",
    "                    return tf.nn.sigmoid(arg) \n",
    "                def relu_fun(arg):\n",
    "                    return tf.nn.relu(arg) \n",
    "                def no_fun(arg):\n",
    "                    return arg\n",
    "\n",
    "            #encoding part of model\n",
    "            with tf.name_scope(\"encoding\"):\n",
    "                #calculate input\n",
    "                linearin = tf.add(tf.matmul(add_noise(self.x,self.noisexsigma),self.win),self.bias) #add noise to input, and multiply by weights\n",
    "                yin = tf.case({tf.equal(self.act_fun,'tanh'): (lambda: tanh_fun(linearin)),\n",
    "                               tf.equal(self.act_fun,'sigmoid'): (lambda: sigmoid_fun(linearin)),\n",
    "                               tf.equal(self.act_fun,'relu'): (lambda: relu_fun(linearin))},\n",
    "                              default=(lambda: no_fun(linearin)),\n",
    "                              exclusive=True)\n",
    "                self.yin = add_noise(yin,self.noisersigma)\n",
    "\n",
    "\n",
    "            #output part of model\n",
    "            with tf.name_scope(\"decoding\"):\n",
    "                #calculate output (reconstruction)\n",
    "                linearout = tf.matmul(self.yin,tf.transpose(self.win)) #add noise to inner layer, and multiply by weight transpose\n",
    "                self.xp = tf.case({tf.equal(self.act_fun,'tanh'): (lambda: tanh_fun(linearout)),\n",
    "                                    tf.equal(self.act_fun,'sigmoid'): (lambda: sigmoid_fun(linearout)),\n",
    "                                    tf.equal(self.act_fun,'relu'): (lambda: relu_fun(linearout))},\n",
    "                                    default=(lambda: no_fun(linearout)),\n",
    "                                    exclusive=True, name='output_nonlienarity')\n",
    "\n",
    "            #calculate cost\n",
    "            with tf.name_scope(\"cost_function\"):\n",
    "                self.cost = tf.sqrt(tf.reduce_mean(tf.square(self.x-self.xp))) - tf.reduce_sum(tf.abs(self.yin*self.lambd))\n",
    "\n",
    "            #train our model\n",
    "            with tf.name_scope(\"training_step\"):\n",
    "                self.train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(self.cost)\n",
    "\n",
    "            # create a summary for our cost, im, reconstruction, & weights\n",
    "            with tf.name_scope('cost_viz'):\n",
    "                tf.summary.scalar(\"cost\", self.cost)\n",
    "\n",
    "            with tf.name_scope('image_viz'):    \n",
    "                x_t = tf.reshape(self.x,(self.params['batchsize'],self.params['imxlen'],self.params['imylen'],1))\n",
    "                tf.summary.image(\"image\", x_t, max_outputs=self.params[\"batchsize\"])\n",
    "\n",
    "            with tf.name_scope('recon_viz'):\n",
    "                xp_t = tf.reshape(self.xp,(self.params['batchsize'],self.params['imxlen'],self.params['imylen'],1))\n",
    "                tf.summary.image(\"recon\", xp_t,max_outputs=self.params[\"batchsize\"])\n",
    "\n",
    "            with tf.name_scope('weights_viz'):    \n",
    "                win_t = tf.reshape(tf.transpose(self.win),\n",
    "                                   (self.params['nneurons'],\n",
    "                                    self.params['imxlen'],\n",
    "                                    self.params['imylen'],1))\n",
    "                tf.summary.image(\"weights\", win_t, max_outputs=self.params['nneurons'])\n",
    "\n",
    "            # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "        return(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load in images \n",
    "def loadimages(psz):\n",
    "    print(\"Loading Van Hateren Natural Image Database...\")\n",
    "    vhimgs = imr.vanHateren(\n",
    "        img_dir='../vanHaterenNaturalImages/VanHaterenNaturalImagesCurated.h5',\n",
    "        #normalize=True,\n",
    "        normalize_im = True,\n",
    "        normalize_patch = False,\n",
    "        invert_colors = False,\n",
    "        patch_edge_size=psz\n",
    "        )\n",
    "    print(\"Done Loading!\")    \n",
    "    np.random.shuffle(vhimgs.images)\n",
    "    print(\"Done Shuffling!\")\n",
    "    return(vhimgs, psz)\n",
    "\n",
    "\n",
    "def check_n_load_ims(psz, iterations):\n",
    "    try:\n",
    "        vhimgs\n",
    "    except NameError:\n",
    "        vhimgs, loadedpatchsize = loadimages(psz)\n",
    "\n",
    "    if(patchsize != loadedpatchsize):\n",
    "        vhimgs, loadedpatchsize = loadimages(psz)\n",
    "\n",
    "    print(\"Images Loaded.\")\n",
    "\n",
    "    #params of images\n",
    "    imxlen = len(vhimgs.images[0,0,:])\n",
    "    imylen = len(vhimgs.images[0,:,0])\n",
    "    nimages = len(vhimgs.images[:,0,0])\n",
    "\n",
    "    nimstrained = batchsize * iterations\n",
    "\n",
    "    if(nimstrained > nimages):\n",
    "        print('ERROR! Trying to train',nimstrained,'images, but we only have',nimages,'images!')\n",
    "    else:\n",
    "        print('Training',nimstrained,'out of',nimages,'total image patches.')\n",
    "\n",
    "    #show an example image\n",
    "    #plt.imshow(vhimgs.images[100],cmap='gray',interpolation='none')\n",
    "    #plt.colorbar()\n",
    "    \n",
    "    return(vhimgs, psz, imxlen, imylen, nimages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set parameters for parameter sweep\n",
    "\n",
    "#make a dictionary\n",
    "params = {}\n",
    "\n",
    "#parameters constant for all\n",
    "params[\"patchsize\"] = 16\n",
    "params[\"imxlen\"] = params[\"patchsize\"]\n",
    "params[\"imylen\"] = params[\"patchsize\"]\n",
    "params[\"iterations\"] = 1000\n",
    "params[\"epochs\"] = 5\n",
    "\n",
    "#params for sweeping\n",
    "#compressions = [1,5,10,25,50,100]\n",
    "#lambdas = [0,1e-5,1e-4,1e-3]\n",
    "#models = ['tanh','relu','sigmoid','linear']\n",
    "#batchsizes = [10,100,1000]\n",
    "#learning_rates = [0.05,0.01,0.1]\n",
    "#noise_x = [1,1e-1,1e-3,1e-5,0]\n",
    "#noise_r = [1,1e-1,1e-3,1e-5,0]\n",
    "\n",
    "#parameters for sweeping - test one now\n",
    "params[\"compression\"] = 10\n",
    "params[\"lambd\"] = 0\n",
    "params[\"model_type\"] = 'tanh'\n",
    "params[\"batchsize\"] = 10\n",
    "params[\"learning_rate\"] = 0.1\n",
    "params[\"noise_x\"] = 1e-3\n",
    "params[\"noise_r\"] = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batchsize': 10,\n",
       " 'compression': 10,\n",
       " 'epochs': 5,\n",
       " 'imxlen': 16,\n",
       " 'imylen': 16,\n",
       " 'iterations': 1000,\n",
       " 'lambd': 0,\n",
       " 'learning_rate': 0.1,\n",
       " 'model_type': 'tanh',\n",
       " 'noise_r': 0.1,\n",
       " 'noise_x': 0.001,\n",
       " 'patchsize': 16}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing by 10 for a total of 25 neurons\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batchsize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8a522eb62051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#make our model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maec_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#make session and train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1812010f062a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1812010f062a>\u001b[0m in \u001b[0;36mmake_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m#input images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimxlen\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimylen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m#activation function type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batchsize' is not defined"
     ]
    }
   ],
   "source": [
    "#make our model\n",
    "aec = aec_model(params)\n",
    "\n",
    "#make session and train model\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    #initialize vars\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    #summary writer for tensorboard\n",
    "    writer = tf.summary.FileWriter(aec.params['savefolder'],\n",
    "                                   graph=tf.get_default_graph())\n",
    "\n",
    "    #save evolution of system over training\n",
    "    cost_evolution = []\n",
    "    wmean_evolution = []\n",
    "\n",
    "    weights_evolution = []\n",
    "    images = []\n",
    "    recons = []\n",
    "\n",
    "    print('Training {} iterations in {} epochs...'.format(aec.params['iterations'],\n",
    "                                                          aec.params['epochs']))\n",
    "    for epoch in range(aec.params['epochs']):\n",
    "        print('\\nEpoch {}: '.format(epoch+1))\n",
    "        np.random.shuffle(vhimgs.images)\n",
    "        for ii in range(aec.params['iterations']):\n",
    "\n",
    "            #reshape our images for feeding to dict\n",
    "            image = np.reshape(vhimgs.images[ii*aec.params['batchsize']:(1+ii)*aec.params['batchsize'],:,:],\n",
    "                               (aec.params['batchsize'],\n",
    "                                aec.params['imxlen']*aec.params['imylen'])).astype(np.float32)\n",
    "\n",
    "            print(np.shape(image))\n",
    "            #setup params to send to dictionary\n",
    "            feeddict = {aec.x: image}\n",
    "#                        aec.params['model_type']: params['model_type'],\n",
    "#                        aec.params['noise_x']: params['noise_x'],\n",
    "#                        aec.params['noise_r']: params['noise_r'],\n",
    "#                        aec.params['lambd']: params['lambd'],\n",
    "#                        aec.params['learning_rate']: params['learning_rate']\n",
    "#                         }\n",
    "\n",
    "            #run our session\n",
    "            sess.run(aec.train_step, feed_dict=feeddict)\n",
    "\n",
    "            #save evolution of params\n",
    "            objcost, inws = sess.run([cost, win], feed_dict=feeddict)\n",
    "            cost_evolution.append(objcost)\n",
    "            wmean_evolution.append(np.mean(inws))\n",
    "\n",
    "            #save detailed parameters 10 times over the total evolution\n",
    "            if(ii%(int((nits*epochs)/10))==0):\n",
    "                print(str(ii)+', ',end=\"\")\n",
    "                #dump our params\n",
    "                w, img, recon = sess.run([win,x,xp], feed_dict=feeddict)\n",
    "                #save our weights, image, and reconstruction\n",
    "                weights_evolution.append(w)\n",
    "                images.append(np.reshape(img,[batchsize,imxlen,imylen]))\n",
    "                recons.append(np.reshape(recon,[batchsize,imxlen,imylen]))\n",
    "\n",
    "    #summarize final params\n",
    "    summary, objcost, inws = sess.run([summary_op ,cost, win], feed_dict=feeddict)\n",
    "    cost_evolution.append(objcost)\n",
    "    wmean_evolution.append(np.mean(inws))\n",
    "    final_weights = win\n",
    "    writer.add_summary(summary,ii)\n",
    "    writer.close()\n",
    "\n",
    "print('\\nDone!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(savefolder, model_type, noise_x, noise_r, batchsize, epochs, nits, lambd, learning_rate, compression, imxlen, imylen):\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        graph = make_graph(imxlen,imylen,compression)        \n",
    "        \n",
    "        #initialize vars\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        #summary writer for tensorboard\n",
    "        writer = tf.summary.FileWriter(savefolder, graph=tf.get_default_graph())\n",
    "\n",
    "        #save evolution of system over training\n",
    "        cost_evolution = []\n",
    "        wmean_evolution = []\n",
    "\n",
    "        weights_evolution = []\n",
    "        images = []\n",
    "        recons = []\n",
    "\n",
    "        print('Training {} iterations in {} epochs...'.format(nits, epochs))\n",
    "        for epoch in range(epochs):\n",
    "            print('\\nEpoch {}: '.format(epoch+1))\n",
    "            np.random.shuffle(vhimgs.images)\n",
    "            for ii in range(nits):\n",
    "\n",
    "                #reshape our images for feeding to dict\n",
    "                image = np.reshape(vhimgs.images[ii*batchsize:(1+ii)*batchsize,:,:],(batchsize,imxlen*imylen)).astype(np.float32)\n",
    "                \n",
    "                #setup params to send to dictionary\n",
    "                feeddict = {x: image, \n",
    "                            act_fun: model_type,\n",
    "                            noisexsigma: noise_x,\n",
    "                            noisersigma: noise_r,\n",
    "                            lamb: lambd,\n",
    "                            learning_rate: learning_rate\n",
    "                             }\n",
    "                \n",
    "                #run our session\n",
    "                sess.run(train_step, feed_dict=feeddict)\n",
    "\n",
    "                #save evolution of params\n",
    "                objcost, inws = sess.run([cost, win], feed_dict=feeddict)\n",
    "                cost_evolution.append(objcost)\n",
    "                wmean_evolution.append(np.mean(inws))\n",
    "\n",
    "                #save detailed parameters 10 times over the total evolution\n",
    "                if(ii%(int((nits*epochs)/10))==0):\n",
    "                    print(str(ii)+', ',end=\"\")\n",
    "                    #dump our params\n",
    "                    w, img, recon = sess.run([win,x,xp], feed_dict=feeddict)\n",
    "                    #save our weights, image, and reconstruction\n",
    "                    weights_evolution.append(w)\n",
    "                    images.append(np.reshape(img,[batchsize,imxlen,imylen]))\n",
    "                    recons.append(np.reshape(recon,[batchsize,imxlen,imylen]))\n",
    "\n",
    "        #summarize final params\n",
    "        summary, objcost, inws = sess.run([summary_op ,cost, win], feed_dict=feeddict)\n",
    "        cost_evolution.append(objcost)\n",
    "        wmean_evolution.append(np.mean(inws))\n",
    "        final_weights = win\n",
    "        writer.add_summary(summary,ii)\n",
    "        writer.close()\n",
    "\n",
    "    print('\\nDone!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'savefolder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-016e8cee193c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#savefolder = diru.make_savefolder(model_type, compression, noise_x, noise_r, lambd)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#vhimgs, psz, imxlen, imylen, nimages = check_n_load_ims(patchsize, iterations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavefolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimylen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'savefolder' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#savefolder = diru.make_savefolder(model_type, compression, noise_x, noise_r, lambd)    \n",
    "#vhimgs, psz, imxlen, imylen, nimages = check_n_load_ims(patchsize, iterations)\n",
    "train_model(savefolder, model_type, noise_x, noise_r, batchsize, epochs, iterations, lambd, learning_rate, compression, imxlen, imylen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run our param sweep\n",
    "if(0):\n",
    "    for patchsize in patchsizes:\n",
    "        vhimgs, psz, imxlen, imylen, nimages = check_n_load_ims(patchsize, its)\n",
    "        for compression in compressions:\n",
    "            graph = make_graph(imxlen, imylen, compression)\n",
    "            for model_type in models:\n",
    "                for batchsize in batchsizes:\n",
    "                    for learning_rate in learning_rates:\n",
    "                        for lambd in lambdas:\n",
    "                            run_model(graph, model_type, batchsize, epochs, nits, lambd, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Show our final weights\n",
    "#wr = np.rollaxis(np.reshape(final_weights,(imxlen,imylen,nneurons)),2)\n",
    "#im = plu.display_data_tiled(wr, normalize=False, title=\"final_weights\", prev_fig=None)\n",
    "\n",
    "#Show our final weights\n",
    "weights_evolution_r = np.rollaxis(np.reshape(weights_evolution,(len(weights_evolution),imxlen,imylen,nneurons)),3,1)\n",
    "(f,sa,ai) = plu.display_data_tiled(weights_evolution_r[-1], normalize=False, title=\"final_weights\", prev_fig=None);\n",
    "f.savefig(savefolder+'weights_final.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save figures\n",
    "if(0):\n",
    "    for i in range(len(weights_evolution_r)):\n",
    "        (f,sa,ai) = plu.display_data_tiled(weights_evolution_r[i], normalize=False,title=\"weights_evolving\", prev_fig=None);\n",
    "        f.savefig(savefolder+'/weights_evolution_'+str(i)+'.png'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f2 = plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.subplot(2,1,1,title='Weights')\n",
    "plt.plot(wmean_evolution)\n",
    "\n",
    "plt.subplot(2,1,2,title='Objective')\n",
    "plt.plot(cost_evolution)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "f2.savefig(savefolder+'/cost_weights.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show an example image and reconstruction from the last iteration of learning\n",
    "patchnum = 3\n",
    "\n",
    "plots = 4\n",
    "for i in range(plots):\n",
    "    plt.subplot(plots,2,2*i+1)#,title='Patch')\n",
    "    plt.imshow(images[-1][patchnum+i],cmap='gray',interpolation='none')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(plots,2,2*i+2)#,title='Recon')\n",
    "    plt.imshow(recons[-1][patchnum+i],cmap='gray',interpolation='none')\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(savefolder+'/reconstruction.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plu.plotonoff(weights_evolution_r[-1]);\n",
    "fig.savefig(savefolder+'/on_off_RFs.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tensorboard Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
