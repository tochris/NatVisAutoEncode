{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y_{in} = F_{act}((Im+n_{in} W_{in}) + bias) $$\n",
    "$$ Im^* = F_{act}(y_{in} W_{in}^T) + n_{out} $$\n",
    "\n",
    "$$ Cost = \\sqrt{\\langle|Im-Im^*|\\rangle} + \\lambda \\langle r \\rangle $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils.plotutils as plu\n",
    "import utils.imreadin as imr\n",
    "import utils.animutils as aniu\n",
    "#import utils.dirutils as diru\n",
    "\n",
    "#code to reload\n",
    "#import imp\n",
    "#imp.reload(dir)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs = {'dpi' : 200} #plotting pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class aec_model(object):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "      params = self.add_params(params)\n",
    "      self.params = params\n",
    "      self.make_dirs()\n",
    "      self.graph = self.make_graph()\n",
    "    \n",
    "    def add_params(self, params):   \n",
    "        params['nneurons'] = np.int(params['imxlen']*params['imylen']/params['compression'])\n",
    "        params['savefolder'] = str('c/actfun_'+ params['model_type']+\n",
    "                                   '/compression_'+ str(params['compression'])+\n",
    "                                   '/noise_in_'+ str(params['noise_x'])+\n",
    "                                   '_out_'+ str(params['noise_r'])+\n",
    "                                   '_lambda_'+ str(params['lambd'])+'/')\n",
    "        return(params)\n",
    "        \n",
    "    def make_dirs(self):\n",
    "        if not os.path.exists(self.params['savefolder']):\n",
    "            os.makedirs(self.params['savefolder'])\n",
    "        \n",
    "    def make_graph(self):\n",
    "    \n",
    "        print('Compressing by',self.params['compression'],'for a total of',self.params['nneurons'],'neurons')\n",
    "\n",
    "        #setup our graph\n",
    "        #tf.reset_default_graph()\n",
    "        mygraph = tf.Graph()\n",
    "        with mygraph.as_default():\n",
    "\n",
    "            #input images\n",
    "            with tf.name_scope('input'):\n",
    "                self.x = tf.placeholder(tf.float32, shape=[self.params[\"batchsize\"], \n",
    "                                                           self.params[\"imxlen\"]*self.params[\"imylen\"]])\n",
    "\n",
    "            #activation function type\n",
    "            with tf.name_scope('activation_function'):\n",
    "                self.act_fun = self.params['model_type']\n",
    "\n",
    "            #noises\n",
    "            with tf.name_scope('noises'):\n",
    "                self.noisexsigma = self.params['noise_x']\n",
    "                self.noisersigma = self.params['noise_r']\n",
    "\n",
    "            #function to add noise\n",
    "            with tf.name_scope(\"add_noise\"):\n",
    "                def add_noise(input_layer, std):\n",
    "                    noise = tf.random_normal(shape=tf.shape(input_layer), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "                    return tf.add(input_layer,noise)\n",
    "\n",
    "            #weights\n",
    "            with tf.variable_scope(\"weights\"):\n",
    "                self.win = tf.Variable(tf.random_normal([self.params['imxlen']*self.params['imylen'],\n",
    "                                                         self.params['nneurons']],\n",
    "                                                        dtype=tf.float32,stddev=0.1))\n",
    "                #wout = tf.Variable(tf.random_normal([nneurons,imxlen*imylen],dtype=tf.float32,stddev=0.01),name='weights_out')\n",
    "\n",
    "            #bias\n",
    "            with tf.variable_scope(\"bias\"):\n",
    "                self.bias = tf.Variable(tf.random_normal([self.params['nneurons']],dtype=tf.float32,stddev=0.1))\n",
    "\n",
    "            #lambda\n",
    "            with tf.name_scope('lambda'):\n",
    "                self.lambd = self.params['lambd']\n",
    "\n",
    "            #learning_rate\n",
    "            with tf.name_scope('learning_rate'):\n",
    "                self.learning_rate = self.params['learning_rate']\n",
    "\n",
    "            #nonlienarities\n",
    "            with tf.name_scope(\"nonlienarities\"):\n",
    "                #define nonlinearities\n",
    "                def tanh_fun(arg):\n",
    "                    return tf.nn.tanh(arg) \n",
    "                def sigmoid_fun(arg):\n",
    "                    return tf.nn.sigmoid(arg) \n",
    "                def relu_fun(arg):\n",
    "                    return tf.nn.relu(arg) \n",
    "                def no_fun(arg):\n",
    "                    return arg\n",
    "\n",
    "            #encoding part of model\n",
    "            with tf.name_scope(\"encoding\"):\n",
    "                #calculate input\n",
    "                linearin = tf.add(tf.matmul(add_noise(self.x,self.params['noise_x']),self.win),self.bias) #add noise to input, and multiply by weights\n",
    "                yin = tf.case({tf.equal(self.act_fun,'tanh'): (lambda: tanh_fun(linearin)),\n",
    "                               tf.equal(self.act_fun,'sigmoid'): (lambda: sigmoid_fun(linearin)),\n",
    "                               tf.equal(self.act_fun,'relu'): (lambda: relu_fun(linearin))},\n",
    "                              default=(lambda: no_fun(linearin)),\n",
    "                              exclusive=True)\n",
    "                self.yin = add_noise(yin,self.params['noise_r'])\n",
    "\n",
    "\n",
    "            #output part of model\n",
    "            with tf.name_scope(\"decoding\"):\n",
    "                #calculate output (reconstruction)\n",
    "                linearout = tf.matmul(self.yin,tf.transpose(self.win)) #add noise to inner layer, and multiply by weight transpose\n",
    "                self.xp = tf.case({tf.equal(self.act_fun,'tanh'): (lambda: tanh_fun(linearout)),\n",
    "                                    tf.equal(self.act_fun,'sigmoid'): (lambda: sigmoid_fun(linearout)),\n",
    "                                    tf.equal(self.act_fun,'relu'): (lambda: relu_fun(linearout))},\n",
    "                                    default=(lambda: no_fun(linearout)),\n",
    "                                    exclusive=True, name='output_nonlienarity')\n",
    "\n",
    "            #calculate cost\n",
    "            with tf.name_scope(\"cost_function\"):\n",
    "                self.cost = tf.sqrt(tf.reduce_mean(tf.square(self.x-self.xp))) - tf.reduce_sum(tf.abs(self.yin*self.lambd))\n",
    "\n",
    "            #train our model\n",
    "            with tf.name_scope(\"training_step\"):\n",
    "                self.train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(self.cost)\n",
    "\n",
    "            # create a summary for our cost, im, reconstruction, & weights\n",
    "            with tf.name_scope('cost_viz'):\n",
    "                tf.summary.scalar(\"cost\", self.cost)\n",
    "\n",
    "            with tf.name_scope('image_viz'):    \n",
    "                x_t = tf.reshape(self.x,(self.params['batchsize'],self.params['imxlen'],self.params['imylen'],1))\n",
    "                tf.summary.image(\"image\", x_t, max_outputs=self.params[\"batchsize\"])\n",
    "\n",
    "            with tf.name_scope('recon_viz'):\n",
    "                xp_t = tf.reshape(self.xp,(self.params['batchsize'],self.params['imxlen'],self.params['imylen'],1))\n",
    "                tf.summary.image(\"recon\", xp_t,max_outputs=self.params[\"batchsize\"])\n",
    "\n",
    "            with tf.name_scope('weights_viz'):    \n",
    "                win_t = tf.reshape(tf.transpose(self.win),\n",
    "                                   (self.params['nneurons'],\n",
    "                                    self.params['imxlen'],\n",
    "                                    self.params['imylen'],1))\n",
    "                tf.summary.image(\"weights\", win_t, max_outputs=self.params['nneurons'])\n",
    "\n",
    "            # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "        return(mygraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load in images \n",
    "def loadimages(psz):\n",
    "    print(\"Loading Van Hateren Natural Image Database...\")\n",
    "    vhimgs = imr.vanHateren(\n",
    "        img_dir='../vanHaterenNaturalImages/VanHaterenNaturalImagesCurated.h5',\n",
    "        #normalize=True,\n",
    "        normalize_im = True,\n",
    "        normalize_patch = False,\n",
    "        invert_colors = False,\n",
    "        patch_edge_size=psz\n",
    "        )\n",
    "    print(\"Done Loading!\")    \n",
    "    np.random.shuffle(vhimgs.images)\n",
    "    print(\"Done Shuffling!\")\n",
    "    return(vhimgs, psz)\n",
    "\n",
    "\n",
    "def check_n_load_ims(psz, iterations, batchsize):\n",
    "    try:\n",
    "        vhimgs\n",
    "    except NameError:\n",
    "        vhimgs, loadedpatchsize = loadimages(psz)\n",
    "\n",
    "    if(psz != loadedpatchsize):\n",
    "        vhimgs, loadedpatchsize = loadimages(psz)\n",
    "\n",
    "    print(\"Images Loaded.\")\n",
    "\n",
    "    #params of images\n",
    "    imxlen = len(vhimgs.images[0,0,:])\n",
    "    imylen = len(vhimgs.images[0,:,0])\n",
    "    nimages = len(vhimgs.images[:,0,0])\n",
    "\n",
    "    nimstrained = batchsize * iterations\n",
    "\n",
    "    if(nimstrained > nimages):\n",
    "        print('ERROR! Trying to train',nimstrained,'images, but we only have',nimages,'images!')\n",
    "    else:\n",
    "        print('Training',nimstrained,'out of',nimages,'total image patches.')\n",
    "\n",
    "    #show an example image\n",
    "    #plt.imshow(vhimgs.images[100],cmap='gray',interpolation='none')\n",
    "    #plt.colorbar()\n",
    "    \n",
    "    return(vhimgs, nimages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Van Hateren Natural Image Database...\n",
      "normalizing full images...\n",
      "sectioning into patches....\n",
      "Done Loading!\n",
      "Done Shuffling!\n",
      "Images Loaded.\n",
      "Training 10000 out of 4685824 total image patches.\n"
     ]
    }
   ],
   "source": [
    "#set parameters for parameter sweep\n",
    "\n",
    "#make a dictionary\n",
    "params = {}\n",
    "\n",
    "#parameters constant for all\n",
    "params[\"patchsize\"] = 16\n",
    "params[\"imxlen\"] = params[\"patchsize\"]\n",
    "params[\"imylen\"] = params[\"patchsize\"]\n",
    "params[\"iterations\"] = 1000\n",
    "params[\"epochs\"] = 5\n",
    "\n",
    "#params for sweeping\n",
    "#compressions = [1,5,10,25,50,100]\n",
    "#lambdas = [0,1e-5,1e-4,1e-3]\n",
    "#models = ['tanh','relu','sigmoid','linear']\n",
    "#batchsizes = [10,100,1000]\n",
    "#learning_rates = [0.05,0.01,0.1]\n",
    "#noise_x = [1,1e-1,1e-3,1e-5,0]\n",
    "#noise_r = [1,1e-1,1e-3,1e-5,0]\n",
    "\n",
    "#parameters for sweeping - test one now\n",
    "params[\"compression\"] = 10\n",
    "params[\"lambd\"] = 0\n",
    "params[\"model_type\"] = 'tanh'\n",
    "params[\"batchsize\"] = 10\n",
    "params[\"learning_rate\"] = 0.1\n",
    "params[\"noise_x\"] = 1e-3\n",
    "params[\"noise_r\"] = 1e-1\n",
    "\n",
    "vhimgs, params['nimages'] = check_n_load_ims(params['patchsize'], params['iterations'], params['batchsize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#make session and train model\n",
    "def train_model(aec):\n",
    "    with tf.Session(graph = aec.graph) as sess:\n",
    "\n",
    "        #initialize vars\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        #summary writer for tensorboard\n",
    "        writer = tf.summary.FileWriter(aec.params['savefolder'],\n",
    "                                       graph=tf.get_default_graph())\n",
    "\n",
    "        #save evolution of system over training\n",
    "        cost_evolution = []\n",
    "        wmean_evolution = []\n",
    "\n",
    "        weights_evolution = []\n",
    "        images = []\n",
    "        recons = []\n",
    "\n",
    "        print('Training {} iterations in {} epochs...'.format(aec.params['iterations'],\n",
    "                                                              aec.params['epochs']))\n",
    "        for epoch in range(aec.params['epochs']):\n",
    "            print('\\nEpoch {}: '.format(epoch+1))\n",
    "            np.random.shuffle(vhimgs.images)\n",
    "            for ii in range(aec.params['iterations']):\n",
    "\n",
    "                #reshape our images for feeding to dict\n",
    "                image = np.reshape(vhimgs.images[ii*aec.params['batchsize']:(1+ii)*aec.params['batchsize'],:,:],\n",
    "                                   (aec.params['batchsize'],\n",
    "                                    aec.params['imxlen']*aec.params['imylen'])).astype(np.float32)\n",
    "\n",
    "                #setup params to send to dictionary\n",
    "                feeddict = {aec.x: image}\n",
    "    #                        aec.params['model_type']: params['model_type'],\n",
    "    #                        aec.params['noise_x']: params['noise_x'],\n",
    "    #                        aec.params['noise_r']: params['noise_r'],\n",
    "    #                        aec.params['lambd']: params['lambd'],\n",
    "    #                        aec.params['learning_rate']: params['learning_rate']\n",
    "    #                        }\n",
    "\n",
    "                #run our session\n",
    "                sess.run(aec.train_step, feed_dict=feeddict)\n",
    "\n",
    "                #save evolution of params\n",
    "                objcost, inws = sess.run([aec.cost, aec.win], feed_dict=feeddict)\n",
    "                cost_evolution.append(objcost)\n",
    "                wmean_evolution.append(np.mean(inws))\n",
    "\n",
    "                #save detailed parameters 10 times over the total evolution\n",
    "                if(ii%(int((aec.params['iterations']*aec.params['epochs'])/10))==0):\n",
    "                    print(str(ii)+', ',end=\"\")\n",
    "                    #dump our params\n",
    "                    w, img, recon = sess.run([aec.win,aec.x,aec.xp], feed_dict=feeddict)\n",
    "                    #save our weights, image, and reconstruction\n",
    "                    weights_evolution.append(w)\n",
    "                    imshape = [aec.params['batchsize'],\n",
    "                              aec.params['imxlen'],\n",
    "                              aec.params['imylen']]\n",
    "\n",
    "                    images.append(np.reshape(img, imshape))\n",
    "                    recons.append(np.reshape(recon, imshape))\n",
    "\n",
    "        #summarize final params\n",
    "        summary, objcost, inws = sess.run([aec.summary_op, aec.cost, aec.win], feed_dict=feeddict)\n",
    "        cost_evolution.append(objcost)\n",
    "        wmean_evolution.append(np.mean(inws))\n",
    "        final_weights = aec.win\n",
    "        writer.add_summary(summary,ii)\n",
    "        writer.close()\n",
    "\n",
    "    print('\\nDone!')\n",
    "    \n",
    "    return(cost_evolutionm,\n",
    "            wmean_evolution,\n",
    "            weights_evolution,\n",
    "            images,\n",
    "            recons,\n",
    "            final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing by 10 for a total of 25 neurons\n",
      "Training 1000 iterations in 5 epochs...\n",
      "\n",
      "Epoch 1: \n",
      "0, 500, \n",
      "Epoch 2: \n"
     ]
    }
   ],
   "source": [
    "#savefolder = diru.make_savefolder(model_type, compression, noise_x, noise_r, lambd)    \n",
    "#make our model\n",
    "aec = aec_model(params)\n",
    "train_model(aec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run our param sweep\n",
    "if(0):\n",
    "    for patchsize in patchsizes:\n",
    "        vhimgs, psz, imxlen, imylen, nimages = check_n_load_ims(patchsize, its)\n",
    "        for compression in compressions:\n",
    "            graph = make_graph(imxlen, imylen, compression)\n",
    "            for model_type in models:\n",
    "                for batchsize in batchsizes:\n",
    "                    for learning_rate in learning_rates:\n",
    "                        for lambd in lambdas:\n",
    "                            run_model(graph, model_type, batchsize, epochs, nits, lambd, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Show our final weights\n",
    "#wr = np.rollaxis(np.reshape(final_weights,(imxlen,imylen,nneurons)),2)\n",
    "#im = plu.display_data_tiled(wr, normalize=False, title=\"final_weights\", prev_fig=None)\n",
    "\n",
    "#Show our final weights\n",
    "weights_evolution_r = np.rollaxis(np.reshape(weights_evolution,(len(weights_evolution),imxlen,imylen,nneurons)),3,1)\n",
    "(f,sa,ai) = plu.display_data_tiled(weights_evolution_r[-1], normalize=False, title=\"final_weights\", prev_fig=None);\n",
    "f.savefig(savefolder+'weights_final.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save figures\n",
    "if(0):\n",
    "    for i in range(len(weights_evolution_r)):\n",
    "        (f,sa,ai) = plu.display_data_tiled(weights_evolution_r[i], normalize=False,title=\"weights_evolving\", prev_fig=None);\n",
    "        f.savefig(savefolder+'/weights_evolution_'+str(i)+'.png'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f2 = plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.subplot(2,1,1,title='Weights')\n",
    "plt.plot(wmean_evolution)\n",
    "\n",
    "plt.subplot(2,1,2,title='Objective')\n",
    "plt.plot(cost_evolution)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "f2.savefig(savefolder+'/cost_weights.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show an example image and reconstruction from the last iteration of learning\n",
    "patchnum = 3\n",
    "\n",
    "plots = 4\n",
    "for i in range(plots):\n",
    "    plt.subplot(plots,2,2*i+1)#,title='Patch')\n",
    "    plt.imshow(images[-1][patchnum+i],cmap='gray',interpolation='none')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(plots,2,2*i+2)#,title='Recon')\n",
    "    plt.imshow(recons[-1][patchnum+i],cmap='gray',interpolation='none')\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(savefolder+'/reconstruction.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plu.plotonoff(weights_evolution_r[-1]);\n",
    "fig.savefig(savefolder+'/on_off_RFs.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tensorboard Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
